{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from infersent import InferSent\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = os.path.join(os.getenv('HOME'), 'fastText', 'crawl-300d-2M.vec')     # check your model location, plz\n",
    "MODEL_PATH = 'encoder/infersent2.pkl'                                           # check your model location, plz\n",
    "BATCH_SIZE = 64\n",
    "EMB_DIM = 300\n",
    "LSTM_DIM = 2048\n",
    "POOL_TYPE = 'max'       # max / mean\n",
    "DPOUT_MODEL = 0.0\n",
    "MODEL_VER = 2\n",
    "\n",
    "params_model = {'bsize': BATCH_SIZE, \n",
    "                'word_emb_dim': EMB_DIM, \n",
    "                'enc_lstm_dim': LSTM_DIM,\n",
    "                'pool_type': POOL_TYPE, \n",
    "                'dpout_model': DPOUT_MODEL, \n",
    "                'version': MODEL_VER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions #\n",
    "def log(content, verbose=True):\n",
    "    if verbose:\n",
    "        print(f'[ LOG ] {content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, config, w2v_path=None):\n",
    "    # load model\n",
    "    model = InferSent(config)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    # model = model.cuda()   # GPU\n",
    "    # model.set_w2v_path(w2v_path)\n",
    "    # model.build_vocab(sentences)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_emb(path):\n",
    "    if not os.path.exists(path):\n",
    "        return None\n",
    "    return np.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @param bundles: list of bundle\n",
    "# @ret: all bundle embedding's np array\n",
    "def get_embeddings(bundles, emb_dir):\n",
    "    # load beginning-of-sent and end-of-sent embedding\n",
    "    emb_bos = np.load(os.path.join(emb_dir, 'bos.npy'))\n",
    "    emb_eos = np.load(os.path.join(emb_dir, 'eos.npy'))\n",
    "    \n",
    "    embeddings = []\n",
    "    lengths = []\n",
    "    bundle_list = []\n",
    "    failures = []\n",
    "    max_len = 0\n",
    "    for bundle in bundles:\n",
    "        words = bundle.split(' ')\n",
    "        emb = []\n",
    "        emb.append(emb_bos)\n",
    "        for w in words:\n",
    "            emb_path = os.path.join(emb_dir, w + '.npy')\n",
    "            emb.append(get_word_emb(emb_path))\n",
    "        emb.append(emb_eos)\n",
    "        if any(e is None for e in emb):\n",
    "            failures.append(bundle)\n",
    "            continue\n",
    "        embeddings.append(emb)\n",
    "        lengths.append(len(emb))\n",
    "        bundle_list.append(bundle)\n",
    "        max_len = len(emb) if len(emb) > max_len else max_len\n",
    "    \n",
    "    batches = np.zeros((max_len, len(embeddings), embeddings[0][0].shape[0]))\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(len(embeddings[i])):\n",
    "            batches[j][i][:] = embeddings[i][j]\n",
    "    return batches, np.array(lengths), bundle_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(bundle_path, model_path, emb_dir, batch_size=64):\n",
    "    log('getting bundles...')\n",
    "    with open(bundle_path, 'r') as f:\n",
    "        bundles = f.read().split('\\n')\n",
    "    embeddings = get_embeddings(bundles, emb_dir)\n",
    "    log(f'{len(bundles)} bundles loaded')\n",
    "\n",
    "    log(f'loading model...')\n",
    "    model = load_model(model_path, config)\n",
    "\n",
    "    for idx in range(0, len(embeddings), batch_size):\n",
    "        batch = torch.FloatTensor(embeddings[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle_path = 'data/bundles_all.txt'\n",
    "model_path = 'encoder/infersent2.pkl'\n",
    "emb_dir = 'word_emb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(bundle_path, 'r') as f:\n",
    "    bundles = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bundles = bundles[:-1]   # exclude spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weekends and holidays',\n",
       " 'wide range of',\n",
       " 'worth a visit',\n",
       " 'worth checking out',\n",
       " 'years ago in']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bundles[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings, lengths = get_embeddings(bundles, emb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 5, 6, 5, 5, 5, 5, 5, 5, 5])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 132, 300)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = params_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_path, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_embs = []\n",
    "for idx in range(0, embeddings.shape[1], batch_size):\n",
    "    batch = torch.FloatTensor(embeddings[:,idx:idx+batch_size, :])\n",
    "    length = lengths[idx:idx+batch_size]\n",
    "    with torch.no_grad():\n",
    "        emb = model.forward((batch, length)).data.cpu().numpy()\n",
    "    p_embs.append(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = np.vstack(p_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 4096)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_bos = np.load(os.path.join(emb_dir, 'bos.npy'))\n",
    "emb_eos = np.load(os.path.join(emb_dir, 'eos.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "failures = []\n",
    "max_len = 0\n",
    "for bundle in bundles:\n",
    "    words = bundle.split(' ')\n",
    "    emb = []\n",
    "    emb.append(emb_bos)\n",
    "    for w in words:\n",
    "        emb_path = os.path.join(emb_dir, w + '.npy')\n",
    "        emb.append(get_word_emb(emb_path))\n",
    "    emb.append(emb_eos)\n",
    "    if any(e is None for e in emb):\n",
    "        failures.append(bundle)\n",
    "        continue\n",
    "    embeddings.append(emb)\n",
    "    max_len = len(emb) if len(emb) > max_len else max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    batch = np.zeros((max_len, len(embeddings), embeddings[0][0].shape[0]))\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(len(embeddings[i])):\n",
    "            batch[j][i][:] = embeddings[i][j]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[0][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = np.zeros((max_len, len(embeddings), embeddings[0][0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 132, 300)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(embeddings)):\n",
    "    for j in range(len(embeddings[i])):\n",
    "        batch[j][i][:] = embeddings[i][j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.3398,  0.301 ,  0.1689, -0.059 ,  0.31  , -0.1625,  0.3938,\n",
       "       -0.4378,  0.038 ,  0.0879,  0.26  ,  1.5207, -0.0033,  0.0499,\n",
       "       -0.1864,  0.3068,  0.1511,  0.5598,  0.131 , -0.3409,  0.1883,\n",
       "       -0.2301,  0.1512, -0.0274, -0.0101,  0.0052, -0.1908,  0.062 ,\n",
       "        0.7689, -0.0926,  0.0714, -0.332 , -0.1068,  0.2163, -0.456 ,\n",
       "        0.318 , -0.0541, -0.1677,  0.1038,  0.008 ,  0.4726, -0.2714,\n",
       "        0.2154,  0.0421, -0.0844, -0.0081, -0.2349,  0.2663, -0.3735,\n",
       "       -0.194 ,  0.1594,  0.3434,  0.8196,  0.2394,  0.0417,  0.4827,\n",
       "        0.1411,  0.1159, -0.0286,  0.0492, -0.2025,  0.4332,  0.1325,\n",
       "        0.064 ,  0.8302,  0.3763, -0.201 , -0.1348,  0.0174, -0.1784,\n",
       "       -0.3994,  0.2344,  0.1994, -0.1032,  0.248 ,  0.2627, -0.2558,\n",
       "        0.1891,  0.2943,  0.049 ,  0.033 ,  0.0905, -0.2289, -0.5167,\n",
       "        0.0826, -0.0607, -0.2633, -0.1619,  0.7178, -0.0209,  0.0718,\n",
       "        0.2381, -0.1027,  0.1029, -0.6198, -0.223 ,  0.1356,  0.2053,\n",
       "        0.2352,  0.1354,  0.7949, -0.6923,  0.2699, -0.2923,  0.1651,\n",
       "       -0.3767,  0.3126,  0.2855,  0.3354,  0.1461,  0.2127, -0.0338,\n",
       "        0.4513, -0.3003, -1.208 , -0.0311,  0.0829,  0.3028, -0.0202,\n",
       "       -0.2518,  0.0371,  0.1631, -0.0158,  0.1777,  0.6634, -0.9175,\n",
       "       -0.0539, -0.3993,  0.3529, -0.264 ,  0.1462, -0.5294,  0.2433,\n",
       "       -0.0931, -0.0669, -0.3548,  0.2053,  0.4978, -0.4745,  0.494 ,\n",
       "       -0.1805, -0.0347,  0.0421, -0.0068, -0.2228,  0.3198,  0.1682,\n",
       "       -0.4401,  0.1902,  0.0679, -0.1141, -0.2378,  0.4879,  0.2263,\n",
       "        0.4614, -0.0131,  0.025 , -0.3065, -0.036 , -0.1799,  0.78  ,\n",
       "        0.0755,  0.0741, -0.3462, -0.3083, -0.2456, -0.0438, -0.0047,\n",
       "        0.356 , -0.495 ,  0.488 , -0.5847, -0.0497, -0.0101, -0.1225,\n",
       "        0.6367, -0.1328, -0.181 , -0.0263, -0.0669, -0.6379, -0.1463,\n",
       "        0.086 ,  0.0244, -0.0842, -0.1739,  0.0902, -0.2007, -0.0139,\n",
       "       -0.5181,  0.1364,  0.1447,  0.2444,  0.3489,  0.1435, -0.5015,\n",
       "        0.4063,  0.0304, -0.2258,  0.1727,  0.0906, -0.7766, -0.1856,\n",
       "        0.1307, -0.115 , -0.0109,  0.4014,  0.1739, -0.3009, -0.0087,\n",
       "        0.6787,  0.8298, -0.1185,  0.1659,  0.4622, -0.2566, -0.2785,\n",
       "        0.6137,  0.0285,  0.2683, -0.1198,  0.4169,  0.0392,  0.0272,\n",
       "       -0.1249, -0.219 ,  0.1121,  0.6083, -0.2509, -0.2425,  0.0338,\n",
       "        0.2602, -0.2274, -0.4608,  0.4267, -0.707 ,  0.4375,  0.3679,\n",
       "       -0.8748,  0.0537, -0.1353,  0.1775, -0.1734, -0.0056,  0.1624,\n",
       "        0.5441, -1.1878, -0.0433,  0.2081, -0.0749, -1.0164, -0.1635,\n",
       "        0.2998,  0.0313, -0.4359, -0.1817,  0.5906, -0.3981,  0.9386,\n",
       "       -0.4054, -0.5032,  0.0796,  0.0395, -0.2277,  0.1454, -0.0117,\n",
       "       -0.2475,  0.3709,  0.3058, -1.5035,  0.4582, -0.3009, -0.1041,\n",
       "        0.0852,  0.7375,  0.3523,  0.1861, -0.1122,  0.1546,  0.2955,\n",
       "       -0.3541, -0.341 ,  0.2006,  0.2671,  0.0702,  0.1371,  0.0231,\n",
       "        0.1769,  0.0455, -0.5545,  0.8907, -0.1838, -0.2747, -0.4761,\n",
       "        0.0652, -0.3084, -0.2821,  0.0639,  0.012 , -0.1761],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_bos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.0100e-01,  3.2120e-01, -2.7000e-02,  2.3670e-01,  9.2500e-02,\n",
       "       -1.6900e-01,  2.6030e-01,  8.6500e-02,  1.3430e-01,  2.6290e-01,\n",
       "       -4.5400e-02,  1.0735e+00,  5.9400e-02,  1.4350e-01, -2.1810e-01,\n",
       "        5.9800e-01,  3.1390e-01,  2.8100e-01, -2.1340e-01, -9.7800e-02,\n",
       "        4.5400e-01, -3.5600e-02,  4.3600e-01,  1.9600e-02, -2.5890e-01,\n",
       "        4.7900e-02, -9.9000e-02, -3.0880e-01,  4.2010e-01,  8.0000e-03,\n",
       "        1.5730e-01, -4.9630e-01,  6.4100e-02, -2.2880e-01, -7.9750e-01,\n",
       "       -2.2830e-01, -2.9000e-02, -5.9100e-01, -1.1740e-01,  4.2200e-02,\n",
       "        8.3000e-03,  2.4720e-01,  4.6340e-01, -1.7100e-02,  1.6050e-01,\n",
       "       -1.9480e-01, -2.7890e-01,  2.4640e-01, -5.0350e-01, -1.2890e-01,\n",
       "        4.1980e-01,  5.0280e-01,  7.7190e-01,  5.1000e-03,  1.4780e-01,\n",
       "        7.3530e-01,  1.6720e-01,  2.2220e-01,  1.2980e-01,  9.7100e-02,\n",
       "       -3.1400e-02,  6.8010e-01,  1.7010e-01, -4.9820e-01,  1.2317e+00,\n",
       "        5.3420e-01, -3.5830e-01,  3.6420e-01, -7.2000e-02, -3.5580e-01,\n",
       "        1.8300e-02,  1.2120e-01,  2.6360e-01,  7.7100e-02, -5.4200e-02,\n",
       "        9.8700e-02, -8.3750e-01, -1.0450e-01, -7.1410e-01,  3.0920e-01,\n",
       "        3.8120e-01, -3.6600e-02, -3.5200e-02, -2.8340e-01,  2.3670e-01,\n",
       "       -2.6710e-01,  2.5320e-01, -4.1730e-01,  6.4930e-01, -1.9440e-01,\n",
       "        1.3110e-01,  1.0430e-01, -7.4900e-02,  6.7600e-02, -5.0880e-01,\n",
       "       -1.1000e-03,  1.1340e-01,  3.8370e-01,  2.0600e-02,  2.0390e-01,\n",
       "        8.8110e-01, -8.8200e-01, -3.3070e-01, -3.2740e-01,  3.4780e-01,\n",
       "       -6.1450e-01,  8.7800e-02, -2.4200e-02,  1.8750e-01,  1.4140e-01,\n",
       "        1.0210e-01,  1.5600e-01,  1.4350e-01, -2.9470e-01, -8.5590e-01,\n",
       "        4.2100e-02,  2.7340e-01,  4.8980e-01, -1.0160e-01, -3.0470e-01,\n",
       "        6.9000e-03,  9.4000e-03, -3.7170e-01,  2.1580e-01,  7.6940e-01,\n",
       "       -6.4180e-01,  9.3000e-02, -5.6200e-01,  2.9600e-01, -3.6600e-02,\n",
       "       -1.5800e-02, -2.1900e-01, -2.9600e-01,  3.2400e-02, -1.2060e-01,\n",
       "       -2.6630e-01,  3.2060e-01,  1.1690e-01, -7.7630e-01,  2.8350e-01,\n",
       "       -2.1210e-01, -1.0110e-01,  5.2000e-03,  1.3350e-01, -5.9000e-03,\n",
       "        4.4210e-01, -1.6600e-02, -5.0140e-01,  1.0810e-01,  4.3090e-01,\n",
       "       -4.7860e-01, -4.2000e-03,  7.0580e-01,  1.7150e-01,  4.6130e-01,\n",
       "       -5.9500e-02, -8.2000e-02, -4.8890e-01, -1.0810e-01, -1.8700e-02,\n",
       "        7.0100e-01, -9.7400e-02,  1.0400e-01, -3.4350e-01, -3.7400e-02,\n",
       "       -1.6240e-01, -1.6090e-01,  1.7150e-01,  3.5550e-01, -3.6390e-01,\n",
       "        3.4100e-01, -6.4590e-01,  4.3100e-02,  3.6500e-01, -2.7510e-01,\n",
       "        3.9490e-01, -9.4900e-02, -2.6680e-01,  2.5300e-02, -9.6000e-03,\n",
       "        6.1000e-03,  8.8500e-02, -1.0700e-02,  1.8460e-01,  1.3710e-01,\n",
       "       -3.8730e-01, -2.7490e-01, -1.9030e-01, -2.6690e-01, -5.6740e-01,\n",
       "        2.4230e-01,  1.0400e-01,  5.3090e-01,  3.6240e-01, -1.3030e-01,\n",
       "       -5.0010e-01,  2.8250e-01, -3.7000e-03, -2.4000e-02,  1.6520e-01,\n",
       "        2.2790e-01, -7.5830e-01, -7.8440e-01, -2.2600e-01, -2.4980e-01,\n",
       "       -3.1820e-01,  4.6330e-01,  9.7600e-02, -1.9100e-01,  1.3020e-01,\n",
       "        3.8820e-01,  1.2557e+00, -3.3700e-01, -4.2120e-01,  8.5820e-01,\n",
       "       -3.6000e-01, -2.2380e-01,  7.2490e-01,  1.6520e-01,  3.7500e-01,\n",
       "       -3.1370e-01,  2.1540e-01, -4.0860e-01,  4.3940e-01,  1.5360e-01,\n",
       "       -5.6800e-01,  3.3220e-01,  4.6490e-01, -3.8520e-01, -4.4100e-01,\n",
       "       -2.0000e-01,  1.9430e-01,  4.2600e-02, -6.8720e-01,  2.9280e-01,\n",
       "       -1.0080e-01,  5.0330e-01,  3.8590e-01, -1.1207e+00, -2.0400e-02,\n",
       "        1.2900e-02, -6.9100e-02, -7.5500e-02,  2.7170e-01,  2.5710e-01,\n",
       "        7.0670e-01, -6.0620e-01, -2.7360e-01,  3.4400e-01,  1.9140e-01,\n",
       "       -8.7150e-01,  1.5580e-01,  2.5220e-01, -3.8330e-01, -1.3650e-01,\n",
       "       -5.4560e-01,  4.1410e-01, -5.6550e-01,  6.9370e-01, -7.2290e-01,\n",
       "       -6.0120e-01,  7.8900e-02,  3.2560e-01, -5.7760e-01, -2.8010e-01,\n",
       "       -2.5000e-02, -7.8300e-02,  1.2620e-01,  1.1050e-01, -1.7507e+00,\n",
       "        6.0190e-01, -2.2290e-01, -3.6230e-01,  1.5450e-01,  1.0893e+00,\n",
       "        5.7280e-01, -8.6700e-02, -8.0000e-02,  5.2350e-01,  3.4800e-02,\n",
       "       -3.0730e-01,  1.4340e-01,  4.3220e-01,  4.6970e-01,  3.6910e-01,\n",
       "        2.4860e-01,  9.1700e-02,  6.5300e-02,  2.4940e-01, -2.2800e-01,\n",
       "        1.0891e+00, -6.4210e-01, -4.5960e-01,  7.3210e-01, -1.0760e-01,\n",
       "       -4.7020e-01, -1.5640e-01,  1.6670e-01, -9.8200e-02, -1.8600e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_eos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3398    ,  0.301     ,  0.1689    , ...,  0.0639    ,\n",
       "         0.012     , -0.1761    ],\n",
       "       [ 0.25195312, -0.19140625, -0.12158203, ..., -0.1171875 ,\n",
       "        -0.15625   , -0.19628906],\n",
       "       [ 0.17675781,  0.02929688,  0.01031494, ..., -0.22265625,\n",
       "        -0.01416016, -0.08935547],\n",
       "       [-0.01177979, -0.04736328,  0.04467773, ...,  0.07128906,\n",
       "        -0.03491211,  0.02416992],\n",
       "       [-0.20100001,  0.32120001, -0.027     , ...,  0.16670001,\n",
       "        -0.0982    , -0.0186    ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 [travel_pattern]",
   "language": "python",
   "name": "travel_pattern"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
